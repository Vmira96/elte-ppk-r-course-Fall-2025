---
title: "TidyTuesday 2025-03-11 - Pixar films"
author: "Vikár Míra"
output:
  html_document:
    df_print: paged
    self_contained: false
knit: rmarkdown::render
---

In this project, I examine data from Pixar films.  
My goal is to explore data patterns (EDA) and then build, compare, and evaluate at least two models.  
The data analysis is fully reproducible, and all code is available in Rmd.

# Load packages

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(tidyverse)
library(readr)
```

```{r}
# Importing the Pixar dataset directly from GitHub

pixar_films <- readr::read_csv(
  "https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-03-11/pixar_films.csv"
)

public_response <- readr::read_csv(
  "https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-03-11/public_response.csv"
)
```

# Pixar dataset head and glimpse
```{r}
head(pixar_films)
head(public_response)
knitr::kable(head(pixar_films))
knitr::kable(head(public_response))
glimpse(pixar_films)
glimpse(public_response)
```

## 1. Data cleaning
#Examining variable types and missing data
```{r}
glimpse(pixar_films)
summary(pixar_films)

glimpse(public_response)
summary(public_response)
```

# Exemination of missing values
```{r}
public_response %>%
  filter(is.na(critics_choice) | is.na(rotten_tomatoes))
```

Data Overview and Initial Cleaning Steps
Before starting the analysis, I examined the dataset to understand its structure and identify potential data quality issues. The dataset contains 24 Pixar films and includes the following variables:

film (character) – The title of the movie.

rotten_tomatoes (numeric) – Percentage score from Rotten Tomatoes.

metacritic (numeric) – Metacritic critic score (0–100).

cinema_score (character) – Audience grade (A, A+, etc.).

critics_choice (numeric) – Critics’ Choice rating (0–100).

Summary of Key Findings
A first look using glimpse() and summary() revealed the following:

All variables were loaded correctly with the expected data types.

cinema_score is stored as a character variable, not numeric, because it contains letter-based grades (e.g., A, A+).

Some variables include missing values (NA):

critics_choice: 3 missing values

metacritic: 1 missing value

cinema_score: 1 missing value

I also inspected rows containing missing data. These came exclusively from the film “Luca” and two early films where Critics Choice ratings were not available.

Next Step: Data Cleaning Plan
Since the dataset is small and missing values are minimal, carefully handling NAs is important. 

# Handling Missing Data

Before proceeding with the exploratory data analysis (EDA), it is essential to
examine the dataset for missing values. Missing data can influence summary
statistics, distort visualisations, and potentially introduce bias during
model building. Therefore, identifying and addressing these values is a
necessary first step.

Treatment Strategy
Because the missingness is low in proportion and the affected variables serve
different analytical purposes, we apply the following approach:

cinema_score:
We convert this variable into an ordered factor with levels
A- < A < A+. The single missing value is left as NA, as imputing an
ordered rating would introduce artificial information.

# Convert cinema_score to an ordered factor
```{r}
public_response <- public_response %>%
  mutate(
    cinema_score = factor(
      cinema_score,
      levels = c("A-", "A", "A+"),   
      ordered = TRUE
    )
  )

```

critics_choice:
Since this is a numerical rating derived from awards evaluation, imputing
a synthetic score would not be justified. These values are therefore
retained as NA and will be handled appropriately during later modelling
(e.g., filtering out incomplete rows or using models that support missingness).

Verification
After these transformations, we confirm that the missing values remain only in
the intended variables:

#Checking the type of variable
```{r}
str(public_response$cinema_score)
```
#Checking values and sequences
```{r}
levels(public_response$cinema_score)
```

# Check missing values again
```{r}
public_response %>% summarise(across(everything(), ~ sum(is.na(.))))
```  
This ensures that the dataset is clean, consistent, and properly structured
for the next steps in the analysis.

## 2. Exploratory Data Analysis (EDA)
- legalább **két ember számára érthető, jól feliratozott plot**
- rövid értelmezés a grafikák alatt
- táblázatok és statisztikák

###2.1 Examination of Rotten Tomatoes distribution
```{r}
public_response %>%
  ggplot(aes(x = rotten_tomatoes)) +
  geom_histogram(binwidth = 5, fill = "#69b3a2") +
  labs(title = "Distribution of Rotten Tomatoes Scores",
       x = "Rotten Tomatoes",
       y = "Count")

```
Exploratory Data Analysis – Rotten Tomatoes Distribution (Rewritten)
The distribution of Rotten Tomatoes scores shows a clear right skew, indicating that most films receive relatively high ratings while lower scores are less common. Visual inspection suggests three noticeable clusters:

a small group around 40 points,

a medium‑sized cluster between 68 and 82 points, and

a prominent concentration between 87 and 100 points.

These clusters imply that the rating system, which ranges from 0 to 100, tends to group films into distinct quality tiers, with the majority achieving comparatively strong critical evaluations.


### 2.2 Rotten Tomatoes vs Metacritic scatterplot
Goal: to show how closely critics' scores correlate.

```{r}
ggplot(public_response, aes(rotten_tomatoes, metacritic)) +
  geom_point(color = "#1f78b4", size = 3) +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  labs(title = "Relation Between Rotten Tomatoes and Metacritic Scores")
```

### 2.3 Pearson correlation and significance test
```{r}
cor_test_result <- cor.test(
  public_response$rotten_tomatoes,
  public_response$metacritic,
  method = "pearson"
)

cor_test_result

```
Correlation Between Rotten Tomatoes and Metacritic Scores
A scatterplot was used to explore the association between Rotten Tomatoes scores and Metacritic ratings. Visual inspection suggested a clear positive relationship: films with lower Rotten Tomatoes scores (around 40–70) tend to receive relatively modest Metacritic ratings, while films scoring above 90 on Rotten Tomatoes cluster between roughly 76 and 92 on Metacritic. This pattern indicates increasing agreement between the two rating systems as film quality rises.

To quantify this relationship, a Pearson product–moment correlation was calculated. The results revealed a strong, positive, and statistically significant correlation between Rotten Tomatoes and Metacritic scores (r = 0.80, t(21) = 6.17, p < 0.001). The 95% confidence interval [0.58, 0.91] further supports the conclusion that the association is reliably above zero.

Overall, the analysis suggests that the two critic-based rating platforms provide largely consistent evaluations of film quality.


### 2.4 Distribution of run_time
```{r}
ggplot(pixar_films, aes(x = run_time)) +
  geom_histogram(fill = "#0073C2FF", color = "white", bins = 10) +
  labs(title = "Distribution of Film Runtime",
       x = "Runtime (minutes)", y = "Count")

```


### 2.5 Basic descriptive statistics for the length of the film
```{r}
summary_stats <- pixar_films %>% 
  summarise(
    mean_run_time = mean(run_time, na.rm = TRUE),
    median_run_time = median(run_time, na.rm = TRUE),
    sd_run_time = sd(run_time, na.rm = TRUE),
    min_run_time = min(run_time, na.rm = TRUE),
    max_run_time = max(run_time, na.rm = TRUE)
  )

summary_stats

```
The distribution of film run times is approximately normal, with most movies lasting between 80 and 120 minutes. The mean run time is 104.8 minutes (median = 102 minutes, SD = 16.8), indicating that typical Pixar films cluster closely around a roughly 100‑minute length. The minimum run time is 81 minutes, while two substantially longer films extend the range up to 155 minutes. These long outliers create a mild right skew in an otherwise symmetric distribution.

### 2.5 run_time vs Rotten Tomatoes, Metacritic, Critics Choice

```{r}
library(dplyr)
library(tidyr)

ratings_long <- public_response %>%
  pivot_longer(cols = c(rotten_tomatoes, metacritic, critics_choice),
               names_to = "rating_type",
               values_to = "rating")

ggplot(ratings_long %>% left_join(pixar_films, by = "film"),
       aes(x = run_time, y = rating)) +
  geom_point(size = 3) +
  geom_smooth(method = "lm", se = FALSE, color = "black") +
  facet_wrap(~ rating_type, scales = "free_y") +
  labs(
    title = "Runtime vs Ratings (Multiple Sources)",
    x = "Runtime (minutes)",
    y = "Rating"
  ) +
  theme_minimal()


```


### 2.6 run_time vs Rotten Tomatoes, Metacritic, Critics Choice Correlations
```{r}
df <- left_join(pixar_films, public_response, by = "film")

cor.test(df$run_time, df$rotten_tomatoes)
cor.test(df$run_time, df$metacritic)
cor.test(df$run_time, df$critics_choice)

```
Correlation analyses were conducted to examine the association between film run time and three measures of critical response (Rotten Tomatoes, Metacritic, Critics’ Choice). Across all three comparisons, results indicated no statistically significant relationship. Run time showed a weak, negative, and non-significant correlation with Rotten Tomatoes scores (r = –0.22, p = .315), Metacritic scores (r = –0.13, p = .564), and Critics’ Choice ratings (r = –0.13, p = .577). The confidence intervals for all estimates included zero, suggesting that film length is not meaningfully associated with critical evaluations. These findings also align with the scatterplots, which show no clear linear trend.

### 2.6 Frequency of film ratings (film_rating)

```{r}
clean_films <- pixar_films %>%
  filter(film_rating %in% c("G", "PG"))
unique(clean_films$film_rating)

```



```{r}
  ggplot(clean_films, aes(x = film_rating)) +
  geom_bar(fill = "#E69F00") +
  labs(title = "Distribution of Film Ratings",
       x = "Rating",
       y = "Count")

```


### 2.7 Frequency of film ratings and its association with running time
```{r}
#frequency table
table(clean_films$film_rating)

#relative frequency
prop.table(table(clean_films$film_rating))

#Are the running times different between G and PG movies?
wilcox.test(run_time ~ film_rating, data = clean_films)


```
After filtering the dataset to include only the meaningful MPAA categories (“G” and “PG”), I examined whether film rating is associated with differences in runtime. A Wilcoxon rank-sum test showed no statistically significant difference between the two groups (W = 64, p = 0.975). This indicates that Pixar’s G‑rated and PG‑rated movies do not differ in their typical runtimes. In practice, this suggests that film length is unrelated to the assigned age rating, and Pixar maintains similar durations across both categories.


### 2.8 Changes in run time (film length) over time

```{r}
library(lubridate)

pixar_films <- pixar_films %>%
  mutate(release_year = year(release_date))
```


```{r}
model_runtime_year <- lm(run_time ~ release_year, data = pixar_films)
summary(model_runtime_year)

plot(model_runtime_year)

pixar_films %>%
  ggplot(aes(x = release_date, y = run_time)) +
  geom_point(color = "#0072B2", size = 3) +
  geom_smooth(method = "lm", se = TRUE, color = "red") +
  labs(
    title = "Film length in time",
    x = "Date of publication",
    y = "Length (minutes)"
  )
```

A simple linear regression was performed to examine whether Pixar film runtimes have changed across release years. The model shows a significant positive relationship between release year and runtime (β = 1.078, p = 0.0098). This means that, on average, Pixar movies have become about 1.1 minutes longer each year.

The model explains approximately 25.6% of the variance in runtimes (R² = 0.256), which indicates a moderate relationship: release year is a meaningful, but not exclusive, predictor of runtime. The residual standard error (14.78 minutes) suggests that runtimes still vary substantially beyond the trend captured by the model.

Overall, the results support the conclusion that Pixar movies tend to get longer over time, although year alone does not account for all runtime differences.


### 2.9 Trends in critical reviews over time

```{r}
public_response %>%
  left_join(pixar_films, by = "film") %>%
  ggplot(aes(x = release_date, y = rotten_tomatoes)) +
  geom_point(color = "#E69F00", size = 3) +
  geom_smooth(method = "lm", se = TRUE, color = "red") +
  labs(
    title = "Rotten Tomatoes score over time",
    x = "Date of publication",
    y = "RT score"
  )
```


### 2.10 How much does time matter when it comes to movie ratings (G vs. PG)?
```{r}
pixar_films %>%
  ggplot(aes(x = release_date, fill = film_rating)) +
  geom_bar() +
  labs(
    title = "Movie ratings over time",
    x = "Date of publication",
    y = "Number of pieces"
  )
```

## 3. Modellek építése
- legalább **két modell** ugyanarra az outcome változóra
  - pl. lineáris modell, logisztikus, random forest, stb.
- modell-összehasonlítás
  - AIC, RMSE, accuracy stb.
- rövid konklúzió: melyik jobb és miért?


```{r}

```

## 4. Modellek értékelése
- **assumption check + residual diagnostics**
  - pl. QQ plot, residuals vs fitted plot
- értelmezés: mi látszik, rendben vannak-e a feltételek?

```{r}


```

## 5. Következtetések / Report
- összefoglalás emberi nyelven
- mik a tanulságok?
- mit találtál érdekesnek?

```{r}


```

## 6. Kreatív rész (ha szeretnél)
- extra vizualizációk
- extra kérdések
- saját ötletek

```{r}


```
