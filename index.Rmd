---
title: "TidyTuesday 2025-03-11 - Pixar films"
author: "Vikár Míra"
output:
  html_document:
    df_print: paged
    self_contained: false
knit: rmarkdown::render
---

In this project, I examine data from Pixar films.  
My goal is to explore data patterns (EDA) and then build, compare, and evaluate at least two models.  
The data analysis is fully reproducible, and all code is available in Rmd.

# Load packages

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(tidyverse)
library(readr)
```

```{r}
# Importing the Pixar dataset directly from GitHub

pixar_films <- readr::read_csv(
  "https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-03-11/pixar_films.csv"
)

public_response <- readr::read_csv(
  "https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-03-11/public_response.csv"
)
```

# Pixar dataset head and glimpse
```{r}
head(pixar_films)
head(public_response)
knitr::kable(head(pixar_films))
knitr::kable(head(public_response))
glimpse(pixar_films)
glimpse(public_response)
```

## 1. Data cleaning
#Examining variable types and missing data
```{r}
glimpse(pixar_films)
summary(pixar_films)

glimpse(public_response)
summary(public_response)
```

# Exemination of missing values
```{r}
public_response %>%
  filter(is.na(critics_choice) | is.na(rotten_tomatoes))
```



Data Overview and Initial Cleaning Steps
Before starting the analysis, I examined the dataset to understand its structure and identify potential data quality issues. The dataset contains 24 Pixar films and includes the following variables:

film (character) – The title of the movie.

rotten_tomatoes (numeric) – Percentage score from Rotten Tomatoes.

metacritic (numeric) – Metacritic critic score (0–100).

cinema_score (character) – Audience grade (A, A+, etc.).

critics_choice (numeric) – Critics’ Choice rating (0–100).

Summary of Key Findings
A first look using glimpse() and summary() revealed the following:

All variables were loaded correctly with the expected data types.

cinema_score is stored as a character variable, not numeric, because it contains letter-based grades (e.g., A, A+).

Some variables include missing values (NA):

critics_choice: 3 missing values

metacritic: 1 missing value

cinema_score: 1 missing value

I also inspected rows containing missing data. These came exclusively from the film “Luca” and two early films where Critics Choice ratings were not available.

Next Step: Data Cleaning Plan
Since the dataset is small and missing values are minimal, carefully handling NAs is important. 

# Handling Missing Data

Before proceeding with the exploratory data analysis (EDA), it is essential to
examine the dataset for missing values. Missing data can influence summary
statistics, distort visualisations, and potentially introduce bias during
model building. Therefore, identifying and addressing these values is a
necessary first step.

Treatment Strategy
Because the missingness is low in proportion and the affected variables serve
different analytical purposes, we apply the following approach:

cinema_score:
We convert this variable into an ordered factor with levels
A- < A < A+. The single missing value is left as NA, as imputing an
ordered rating would introduce artificial information.

# Convert cinema_score to an ordered factor
```{r}
public_response <- public_response %>%
  mutate(
    cinema_score = factor(
      cinema_score,
      levels = c("A-", "A", "A+"),   
      ordered = TRUE
    )
  )

```



critics_choice:
Since this is a numerical rating derived from awards evaluation, imputing
a synthetic score would not be justified. These values are therefore
retained as NA and will be handled appropriately during later modelling
(e.g., filtering out incomplete rows or using models that support missingness).

Verification
After these transformations, we confirm that the missing values remain only in
the intended variables:

#Checking the type of variable
```{r}
str(public_response$cinema_score)
```
#Checking values and sequences
```{r}
levels(public_response$cinema_score)
```

# Check missing values again
```{r}
public_response %>% summarise(across(everything(), ~ sum(is.na(.))))
```  



This ensures that the dataset is clean, consistent, and properly structured
for the next steps in the analysis.

## 2. Exploratory Data Analysis (EDA)
- legalább **két ember számára érthető, jól feliratozott plot**
- rövid értelmezés a grafikák alatt
- táblázatok és statisztikák

###2.1 Examination of Rotten Tomatoes distribution
```{r}
public_response %>%
  ggplot(aes(x = rotten_tomatoes)) +
  geom_histogram(binwidth = 5, fill = "#69b3a2") +
  labs(title = "Distribution of Rotten Tomatoes Scores",
       x = "Rotten Tomatoes",
       y = "Count")

```



Exploratory Data Analysis – Rotten Tomatoes Distribution (Rewritten)
The distribution of Rotten Tomatoes scores shows a clear right skew, indicating that most films receive relatively high ratings while lower scores are less common. Visual inspection suggests three noticeable clusters:

a small group around 40 points,

a medium‑sized cluster between 68 and 82 points, and

a prominent concentration between 87 and 100 points.

These clusters imply that the rating system, which ranges from 0 to 100, tends to group films into distinct quality tiers, with the majority achieving comparatively strong critical evaluations.




### 2.2 Distribution of run_time
```{r}
ggplot(pixar_films, aes(x = run_time)) +
  geom_histogram(fill = "#0073C2FF", color = "white", bins = 10) +
  labs(title = "Distribution of Film Runtime",
       x = "Runtime (minutes)", y = "Count")

```


### 2.3 Basic descriptive statistics for the length of the film
```{r}
summary_stats <- pixar_films %>% 
  summarise(
    mean_run_time = mean(run_time, na.rm = TRUE),
    median_run_time = median(run_time, na.rm = TRUE),
    sd_run_time = sd(run_time, na.rm = TRUE),
    min_run_time = min(run_time, na.rm = TRUE),
    max_run_time = max(run_time, na.rm = TRUE)
  )

summary_stats

```



The distribution of film run times is approximately normal, with most movies lasting between 80 and 120 minutes. The mean run time is 104.8 minutes (median = 102 minutes, SD = 16.8), indicating that typical Pixar films cluster closely around a roughly 100‑minute length. The minimum run time is 81 minutes, while two substantially longer films extend the range up to 155 minutes. These long outliers create a mild right skew in an otherwise symmetric distribution.

### 2.4 run_time vs Rotten Tomatoes, Metacritic, Critics Choice

```{r}
library(dplyr)
library(tidyr)

ratings_long <- public_response %>%
  pivot_longer(cols = c(rotten_tomatoes, metacritic, critics_choice),
               names_to = "rating_type",
               values_to = "rating")

ggplot(ratings_long %>% left_join(pixar_films, by = "film"),
       aes(x = run_time, y = rating)) +
  geom_point(size = 3) +
  geom_smooth(method = "lm", se = FALSE, color = "black") +
  facet_wrap(~ rating_type, scales = "free_y") +
  labs(
    title = "Runtime vs Ratings (Multiple Sources)",
    x = "Runtime (minutes)",
    y = "Rating"
  ) +
  theme_minimal()


```


### 2.5 run_time vs Rotten Tomatoes, Metacritic, Critics Choice Correlations
```{r}
df <- left_join(pixar_films, public_response, by = "film")

cor.test(df$run_time, df$rotten_tomatoes)
cor.test(df$run_time, df$metacritic)
cor.test(df$run_time, df$critics_choice)

```



Correlation analyses were conducted to examine the association between film run time and three measures of critical response (Rotten Tomatoes, Metacritic, Critics’ Choice). Across all three comparisons, results indicated no statistically significant relationship. Run time showed a weak, negative, and non-significant correlation with Rotten Tomatoes scores (r = –0.22, p = .315), Metacritic scores (r = –0.13, p = .564), and Critics’ Choice ratings (r = –0.13, p = .577). The confidence intervals for all estimates included zero, suggesting that film length is not meaningfully associated with critical evaluations. These findings also align with the scatterplots, which show no clear linear trend.

### 2.6 Frequency of film ratings (film_rating)

```{r}
clean_films <- pixar_films %>%
  filter(film_rating %in% c("G", "PG"))
unique(clean_films$film_rating)

```



```{r}
  ggplot(clean_films, aes(x = film_rating)) +
  geom_bar(fill = "#E69F00") +
  labs(title = "Distribution of Film Ratings",
       x = "Rating",
       y = "Count")

```


### 2.7 Frequency of film ratings and its association with running time
```{r}
#frequency table
table(clean_films$film_rating)

#relative frequency
prop.table(table(clean_films$film_rating))

#Are the running times different between G and PG movies?
wilcox.test(run_time ~ film_rating, data = clean_films)


```



After filtering the dataset to include only the meaningful MPAA categories (“G” and “PG”), I examined whether film rating is associated with differences in runtime. A Wilcoxon rank-sum test showed no statistically significant difference between the two groups (W = 64, p = 0.975). This indicates that Pixar’s G‑rated and PG‑rated movies do not differ in their typical runtimes. In practice, this suggests that film length is unrelated to the assigned age rating, and Pixar maintains similar durations across both categories.


### 2.8 Changes in run time (film length) over time

```{r}
library(lubridate)

pixar_films <- pixar_films %>%
  mutate(release_year = year(release_date))
```


```{r}
model_runtime_year <- lm(run_time ~ release_year, data = pixar_films)
summary(model_runtime_year)

plot(model_runtime_year)

pixar_films %>%
  ggplot(aes(x = release_date, y = run_time)) +
  geom_point(color = "#0072B2", size = 3) +
  geom_smooth(method = "lm", se = TRUE, color = "red") +
  labs(
    title = "Film length in time",
    x = "Date of publication",
    y = "Length (minutes)"
  )
```




A simple linear regression was performed to examine whether Pixar film runtimes have changed across release years. The model shows a significant positive relationship between release year and runtime (β = 1.078, p = 0.0098). This means that, on average, Pixar movies have become about 1.1 minutes longer each year.

The model explains approximately 25.6% of the variance in runtimes (R² = 0.256), which indicates a moderate relationship: release year is a meaningful, but not exclusive, predictor of runtime. The residual standard error (14.78 minutes) suggests that runtimes still vary substantially beyond the trend captured by the model.

We should also consider that The scatterplot suggests a very mild upward trend in movie runtimes over time, but the pattern is far from linear. The smoothing curve is wavy, implying that runtimes fluctuate rather than follow a steady increase. Three clear outliers appear at fitted values around 98, 115, and 118 minutes, corresponding to unusually short or long movies compared with their release-year peers.
The residuals-vs-fitted plot shows non‑random structure: the loess curve bends several times, indicating model misspecification and suggesting that a linear model does not fit the data well. Additionally, the Q–Q plot shows noticeable deviations from normality at both ends, driven largely by the same outliers identified earlier.

Overall, the results support the conclusion that Pixar movies tend to get longer over time, although year alone does not account for all runtime differences.



### 2.9 How much does time matter when it comes to movie ratings (G vs. PG)?
```{r}
# Add release_year
clean_films <- clean_films %>%
  mutate(release_year = lubridate::year(release_date))

# Convert rating to binary (PG = 1, G = 0)
clean_films$rating_binary <- ifelse(clean_films$film_rating == "PG", 1, 0)

# Logistic regression: does rating depend on release year?
rating_model <- glm(rating_binary ~ release_year, 
                    data = clean_films, 
                    family = binomial)

summary(rating_model)

pixar_films %>%
  ggplot(aes(x = release_date, fill = film_rating)) +
  geom_bar() +
  labs(
    title = "Movie ratings over time",
    x = "Date of publication",
    y = "Number of pieces"
  )
```



The plot shows how movie ratings have changed over time across the Pixar dataset. Early releases are almost exclusively rated G, but from the late 2000s onward PG-rated films appear more frequently, and they gradually become the dominant category. A few films in the 2020s have missing or “Not Rated” labels, but these represent isolated cases rather than a trend.

To quantify this visually observable shift, I fitted a logistic regression predicting the probability of a movie being PG based on its release year. The model shows a significant positive effect of release year (p = 0.0246), meaning that movies released later are statistically more likely to receive a PG rating.

### 2.10 Trends in critical reviews over time

```{r}
public_response2 <- public_response %>%
  left_join(pixar_films, by = "film") %>%
  mutate(release_year = as.numeric(format(release_date, "%Y")))

rt_model <- lm(rotten_tomatoes ~ release_year, data = public_response2)
summary(rt_model)

public_response %>%
  left_join(pixar_films, by = "film") %>%
  ggplot(aes(x = release_date, y = rotten_tomatoes)) +
  geom_point(color = "#E69F00", size = 3) +
  geom_smooth(method = "lm", se = TRUE, color = "red") +
  labs(
    title = "Rotten Tomatoes score over time",
    x = "Date of publication",
    y = "RT score"
  )
```




The scatterplot shows Rotten Tomatoes scores across release years, together with a fitted linear trend. The distribution of points suggests no strong upward or downward pattern in critical reception over time.

The linear regression model confirms this visually observed result. The slope of the regression line is small and negative (β = –0.40), but not statistically significant (p = 0.33). The model explains almost none of the variance in the scores (adjusted R² ≈ 0). This indicates that release year is not a meaningful predictor of Rotten Tomatoes ratings in this dataset.

Overall, the analysis suggests that Pixar films have maintained relatively stable critical reception over time, without a clear long-term improvement or decline.


### 2.11 Examination of the relationship between assessments and further correlation studies

# Join the two datasets by film name
```{r}
library(dplyr)
library(ggplot2)


pixar_joined <- public_response %>%
  left_join(pixar_films, by = "film")

```

# Rotten Tomatoes vs Metacritic
```{r}
ggplot(pixar_joined, aes(x = rotten_tomatoes, y = metacritic)) +
  geom_point(color = "#E69F00", size = 3) +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  labs(title = "Rotten Tomatoes vs Metacritic",
       x = "Rotten Tomatoes score",
       y = "Metacritic score")
```



# Rotten Tomatoes vs Critics Choice
```{r}
ggplot(pixar_joined, aes(x = rotten_tomatoes, y = critics_choice)) +
  geom_point(color = "#56B4E9", size = 3) +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  labs(title = "Rotten Tomatoes vs Critics Choice",
       x = "Rotten Tomatoes score",
       y = "Critics Choice score")
```

# Metacritic vs Critics Choice
```{r}
ggplot(pixar_joined, aes(x = metacritic, y = critics_choice)) +
  geom_point(color = "#009E73", size = 3) +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  labs(title = "Metacritic vs Critics Choice",
       x = "Metacritic score",
       y = "Critics Choice score")
```


# Calculation of correlations between numerical evaluations
```{r}
numeric_scores <- pixar_joined %>%
  select(rotten_tomatoes, metacritic, critics_choice)

cor_matrix <- cor(numeric_scores, use = "complete.obs")

cor_matrix
```


# Correlation heatmap
```{r}
install.packages("ggcorrplot")
library(ggcorrplot)

merged <- public_response %>%
  left_join(pixar_films, by = "film")

num_vars <- merged %>%
  select(run_time, rotten_tomatoes, metacritic, critics_choice)

corr_matrix <- cor(num_vars, use = "pairwise.complete.obs")

ggcorrplot(corr_matrix,
           lab = TRUE,
           type = "lower",
           hc.order = TRUE,
           colors = c("blue", "white", "red"))
```



The correlation matrix shows strong and positive associations between all three critic‑based rating systems.

Rotten Tomatoes and Metacritic: r = 0.80

Rotten Tomatoes and Critics’ Choice: r = 0.85

Metacritic and Critics’ Choice: r = 0.86

These coefficients indicate that films rated highly by one platform tend to receive similarly high scores on the others. The strength of the correlations (all above 0.80) suggests that the different critic aggregators evaluate Pixar films in a highly consistent manner.

The correlations between runtime and the three critics’ rating variables (Rotten Tomatoes, Metacritic, Critics’ Choice) were all weak and negative (r = –0.22, –0.13, –0.13).
These values indicate a very weak inverse relationship, meaning that longer films tend to receive slightly lower critic scores, but the effect is minimal.
Given the small magnitude of the correlations, these relationships are not statistically significant.
Overall, runtime does not appear to meaningfully influence critical reception for Pixar films.



```{r}

```



## 3. Modellek építése
- legalább **két modell** ugyanarra az outcome változóra
  - pl. lineáris modell, logisztikus, random forest, stb.
- modell-összehasonlítás
  - AIC, RMSE, accuracy stb.
- rövid konklúzió: melyik jobb és miért?


```{r}

```

## 4. Modellek értékelése
- **assumption check + residual diagnostics**
  - pl. QQ plot, residuals vs fitted plot
- értelmezés: mi látszik, rendben vannak-e a feltételek?

```{r}


```

## 5. Következtetések / Report
- összefoglalás emberi nyelven
- mik a tanulságok?
- mit találtál érdekesnek?

```{r}


```

## 6. Kreatív rész (ha szeretnél)
- extra vizualizációk
- extra kérdések
- saját ötletek

```{r}


```
