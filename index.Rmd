---
title: "TidyTuesday 2025-03-11 - Pixar films"
author: "Vikár Míra"
output:
  html_document:
    df_print: paged
    self_contained: false
knit: rmarkdown::render
---

In this project, I examine data from Pixar films.  
My goal is to explore data patterns (EDA) and then build, compare, and evaluate at least two models.  
The data analysis is fully reproducible, and all code is available in Rmd.

# Load packages

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(tidyverse)
library(readr)
```

```{r}
# Importing the Pixar dataset directly from GitHub

pixar_films <- readr::read_csv(
  "https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-03-11/pixar_films.csv"
)

public_response <- readr::read_csv(
  "https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-03-11/public_response.csv"
)
```

# Pixar dataset head and glimpse
```{r}
head(pixar_films)
head(public_response)
knitr::kable(head(pixar_films))
knitr::kable(head(public_response))
glimpse(pixar_films)
glimpse(public_response)
```

## 1. Data cleaning
#Examining variable types and missing data
```{r}
glimpse(pixar_films)
summary(pixar_films)

glimpse(public_response)
summary(public_response)
```

# Exemination of missing values
```{r}
public_response %>%
  filter(is.na(critics_choice) | is.na(rotten_tomatoes))
```



Data Overview and Initial Cleaning Steps
Before starting the analysis, I examined the dataset to understand its structure and identify potential data quality issues. The dataset contains 24 Pixar films and includes the following variables:

film (character) – The title of the movie.

rotten_tomatoes (numeric) – Percentage score from Rotten Tomatoes.

metacritic (numeric) – Metacritic critic score (0–100).

cinema_score (character) – Audience grade (A, A+, etc.).

critics_choice (numeric) – Critics’ Choice rating (0–100).

Summary of Key Findings
A first look using glimpse() and summary() revealed the following:

All variables were loaded correctly with the expected data types.

cinema_score is stored as a character variable, not numeric, because it contains letter-based grades (e.g., A, A+).

Some variables include missing values (NA):

critics_choice: 3 missing values

metacritic: 1 missing value

cinema_score: 1 missing value

I also inspected rows containing missing data. These came exclusively from the film “Luca” and two early films where Critics Choice ratings were not available.

Next Step: Data Cleaning Plan
Since the dataset is small and missing values are minimal, carefully handling NAs is important. 

# Handling Missing Data

Before proceeding with the exploratory data analysis (EDA), it is essential to
examine the dataset for missing values. Missing data can influence summary
statistics, distort visualisations, and potentially introduce bias during
model building. Therefore, identifying and addressing these values is a
necessary first step.

Treatment Strategy
Because the missingness is low in proportion and the affected variables serve
different analytical purposes, we apply the following approach:

cinema_score:
We convert this variable into an ordered factor with levels
A- < A < A+. The single missing value is left as NA, as imputing an
ordered rating would introduce artificial information.

# Convert cinema_score to an ordered factor
```{r}
public_response <- public_response %>%
  mutate(
    cinema_score = factor(
      cinema_score,
      levels = c("A-", "A", "A+"),   
      ordered = TRUE
    )
  )

```



critics_choice:
Since this is a numerical rating derived from awards evaluation, imputing
a synthetic score would not be justified. These values are therefore
retained as NA and will be handled appropriately during later modelling
(e.g., filtering out incomplete rows or using models that support missingness).

Verification
After these transformations, we confirm that the missing values remain only in
the intended variables:

#Checking the type of variable
```{r}
str(public_response$cinema_score)
```
#Checking values and sequences
```{r}
levels(public_response$cinema_score)
```

# Check missing values again
```{r}
public_response %>% summarise(across(everything(), ~ sum(is.na(.))))
```  



This ensures that the dataset is clean, consistent, and properly structured
for the next steps in the analysis.

## 2. Exploratory Data Analysis (EDA)

### 2.1 Distribution of run_time
```{r}
ggplot(pixar_films, aes(x = run_time)) +
  geom_histogram(fill = "#0073C2FF", color = "white", bins = 10) +
  labs(title = "Distribution of Film Runtime",
       x = "Runtime (minutes)", y = "Count")

```


### 2.2 Basic descriptive statistics for the length of the film
```{r}
summary_stats <- pixar_films %>% 
  summarise(
    mean_run_time = mean(run_time, na.rm = TRUE),
    median_run_time = median(run_time, na.rm = TRUE),
    sd_run_time = sd(run_time, na.rm = TRUE),
    min_run_time = min(run_time, na.rm = TRUE),
    max_run_time = max(run_time, na.rm = TRUE)
  )

summary_stats

```



The distribution of film run times is approximately normal, with most movies lasting between 80 and 120 minutes. The mean run time is 104.8 minutes (median = 102 minutes, SD = 16.8), indicating that typical Pixar films cluster closely around a roughly 100‑minute length. The minimum run time is 81 minutes, while two substantially longer films extend the range up to 155 minutes. These long outliers create a mild right skew in an otherwise symmetric distribution.

### 2.3 run_time vs Rotten Tomatoes, Metacritic, Critics Choice

```{r}
library(dplyr)
library(tidyr)

ratings_long <- public_response %>%
  pivot_longer(cols = c(rotten_tomatoes, metacritic, critics_choice),
               names_to = "rating_type",
               values_to = "rating")

ggplot(ratings_long %>% left_join(pixar_films, by = "film"),
       aes(x = run_time, y = rating)) +
  geom_point(size = 3) +
  geom_smooth(method = "lm", se = FALSE, color = "black") +
  facet_wrap(~ rating_type, scales = "free_y") +
  labs(
    title = "Runtime vs Ratings (Multiple Sources)",
    x = "Runtime (minutes)",
    y = "Rating"
  ) +
  theme_minimal()


```


### 2.4 run_time vs Rotten Tomatoes, Metacritic, Critics Choice Correlations
```{r}
df <- left_join(pixar_films, public_response, by = "film")

cor.test(df$run_time, df$rotten_tomatoes)
cor.test(df$run_time, df$metacritic)
cor.test(df$run_time, df$critics_choice)

```



Correlation analyses were conducted to examine the association between film run time and three measures of critical response (Rotten Tomatoes, Metacritic, Critics’ Choice). Across all three comparisons, results indicated no statistically significant relationship. Run time showed a weak, negative, and non-significant correlation with Rotten Tomatoes scores (r = –0.22, p = .315), Metacritic scores (r = –0.13, p = .564), and Critics’ Choice ratings (r = –0.13, p = .577). The confidence intervals for all estimates included zero, suggesting that film length is not meaningfully associated with critical evaluations. These findings also align with the scatterplots, which show no clear linear trend.

### 2.5 Frequency of film ratings (film_rating)

```{r}
clean_films <- pixar_films %>%
  filter(film_rating %in% c("G", "PG"))
unique(clean_films$film_rating)

```



```{r}
  ggplot(clean_films, aes(x = film_rating)) +
  geom_bar(fill = "#E69F00") +
  labs(title = "Distribution of Film Ratings",
       x = "Rating",
       y = "Count")

```


### 2.6 Frequency of film ratings and its association with running time
```{r}
#frequency table
table(clean_films$film_rating)

#relative frequency
prop.table(table(clean_films$film_rating))

#Are the running times different between G and PG movies?
wilcox.test(run_time ~ film_rating, data = clean_films)


```



After filtering the dataset to include only the meaningful MPAA categories (“G” and “PG”), I examined whether film rating is associated with differences in runtime. A Wilcoxon rank-sum test showed no statistically significant difference between the two groups (W = 64, p = 0.975). This indicates that Pixar’s G‑rated and PG‑rated movies do not differ in their typical runtimes. In practice, this suggests that film length is unrelated to the assigned age rating, and Pixar maintains similar durations across both categories.


### 2.7 Changes in run time (film length) over time

```{r}
library(lubridate)

pixar_films <- pixar_films %>%
  mutate(release_year = year(release_date))
```


```{r}
model_runtime_year <- lm(run_time ~ release_year, data = pixar_films)
summary(model_runtime_year)

plot(model_runtime_year)

pixar_films %>%
  ggplot(aes(x = release_date, y = run_time)) +
  geom_point(color = "#0072B2", size = 3) +
  geom_smooth(method = "lm", se = TRUE, color = "red") +
  labs(
    title = "Film length in time",
    x = "Date of publication",
    y = "Length (minutes)"
  )
```




A simple linear regression was performed to examine whether Pixar film runtimes have changed across release years. The model shows a significant positive relationship between release year and runtime (β = 1.078, p = 0.0098). This means that, on average, Pixar movies have become about 1.1 minutes longer each year.

The model explains approximately 25.6% of the variance in runtimes (R² = 0.256), which indicates a moderate relationship: release year is a meaningful, but not exclusive, predictor of runtime. The residual standard error (14.78 minutes) suggests that runtimes still vary substantially beyond the trend captured by the model.

We should also consider that The scatterplot suggests a very mild upward trend in movie runtimes over time, but the pattern is far from linear. The smoothing curve is wavy, implying that runtimes fluctuate rather than follow a steady increase. Three clear outliers appear at fitted values around 98, 115, and 118 minutes, corresponding to unusually short or long movies compared with their release-year peers.
The residuals-vs-fitted plot shows non‑random structure: the loess curve bends several times, indicating model misspecification and suggesting that a linear model does not fit the data well. Additionally, the Q–Q plot shows noticeable deviations from normality at both ends, driven largely by the same outliers identified earlier.

Overall, the results support the conclusion that Pixar movies tend to get longer over time, although year alone does not account for all runtime differences.



### 2.8 How much does time matter when it comes to movie ratings (G vs. PG)?
```{r}
# Add release_year
clean_films <- clean_films %>%
  mutate(release_year = lubridate::year(release_date))

# Convert rating to binary (PG = 1, G = 0)
clean_films$rating_binary <- ifelse(clean_films$film_rating == "PG", 1, 0)

# Logistic regression: does rating depend on release year?
rating_model <- glm(rating_binary ~ release_year, 
                    data = clean_films, 
                    family = binomial)

summary(rating_model)

pixar_films %>%
  ggplot(aes(x = release_date, fill = film_rating)) +
  geom_bar() +
  labs(
    title = "Movie ratings over time",
    x = "Date of publication",
    y = "Number of pieces"
  )
```



The plot shows how movie ratings have changed over time across the Pixar dataset. Early releases are almost exclusively rated G, but from the late 2000s onward PG-rated films appear more frequently, and they gradually become the dominant category. A few films in the 2020s have missing or “Not Rated” labels, but these represent isolated cases rather than a trend.

To quantify this visually observable shift, I fitted a logistic regression predicting the probability of a movie being PG based on its release year. The model shows a significant positive effect of release year (p = 0.0246), meaning that movies released later are statistically more likely to receive a PG rating.

### 2.9 Trends in critical reviews over time

```{r}
public_response2 <- public_response %>%
  left_join(pixar_films, by = "film") %>%
  mutate(release_year = as.numeric(format(release_date, "%Y")))

rt_model <- lm(rotten_tomatoes ~ release_year, data = public_response2)
summary(rt_model)

public_response %>%
  left_join(pixar_films, by = "film") %>%
  ggplot(aes(x = release_date, y = rotten_tomatoes)) +
  geom_point(color = "#E69F00", size = 3) +
  geom_smooth(method = "lm", se = TRUE, color = "red") +
  labs(
    title = "Rotten Tomatoes score over time",
    x = "Date of publication",
    y = "RT score"
  )
```




The scatterplot shows Rotten Tomatoes scores across release years, together with a fitted linear trend. The distribution of points suggests no strong upward or downward pattern in critical reception over time.

The linear regression model confirms this visually observed result. The slope of the regression line is small and negative (β = –0.40), but not statistically significant (p = 0.33). The model explains almost none of the variance in the scores (adjusted R² ≈ 0). This indicates that release year is not a meaningful predictor of Rotten Tomatoes ratings in this dataset.

Overall, the analysis suggests that Pixar films have maintained relatively stable critical reception over time, without a clear long-term improvement or decline.


### 2.10 Examination of the relationship between assessments and further correlation studies

# Join the two datasets by film name
```{r}
library(dplyr)
library(ggplot2)


pixar_joined <- public_response %>%
  left_join(pixar_films, by = "film")

```



# Histogram for all 3 numeric critic scores
```{r}
public_response %>%
  select(rotten_tomatoes, metacritic, critics_choice) %>%
  gather(key = "source", value = "score") %>%
  ggplot(aes(x = score)) +
  geom_histogram(bins = 10, fill = "#E69F00", color = "black") +
  facet_wrap(~ source, scales = "free") +
  labs(title = "Distribution of critic scores",
       x = "Score",
       y = "Count")
```




Across all three critic-based rating variables (Rotten Tomatoes, Metacritic, Critics’ Choice), the score distributions are right‑skewed, indicating that most Pixar films tend to receive high critical evaluations.

Rotten Tomatoes shows the strongest skew: the majority of films score above 90 points, while only a few titles fall below 60 points. Metacritic scores are also skewed but more moderately, with most films positioned between 70 and 90 points. Critics’ Choice ratings follow a similar pattern, with almost all films receiving scores above 85 points, and only a small number falling into the mid‑range (50–70 points).

Overall, the distributions suggest that Pixar films consistently achieve high critical acclaim, with very few low-rated outliers.



# Rotten Tomatoes vs Metacritic
```{r}
ggplot(pixar_joined, aes(x = rotten_tomatoes, y = metacritic)) +
  geom_point(color = "#E69F00", size = 3) +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  labs(title = "Rotten Tomatoes vs Metacritic",
       x = "Rotten Tomatoes score",
       y = "Metacritic score")
```



# Rotten Tomatoes vs Critics Choice
```{r}
ggplot(pixar_joined, aes(x = rotten_tomatoes, y = critics_choice)) +
  geom_point(color = "#56B4E9", size = 3) +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  labs(title = "Rotten Tomatoes vs Critics Choice",
       x = "Rotten Tomatoes score",
       y = "Critics Choice score")
```

# Metacritic vs Critics Choice
```{r}
ggplot(pixar_joined, aes(x = metacritic, y = critics_choice)) +
  geom_point(color = "#009E73", size = 3) +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  labs(title = "Metacritic vs Critics Choice",
       x = "Metacritic score",
       y = "Critics Choice score")
```


# Calculation of correlations between numerical evaluations
```{r}
numeric_scores <- pixar_joined %>%
  select(rotten_tomatoes, metacritic, critics_choice)

cor_matrix <- cor(numeric_scores, use = "complete.obs")

cor_matrix
```


# Correlation heatmap
```{r}
install.packages("ggcorrplot")
library(ggcorrplot)

merged <- public_response %>%
  left_join(pixar_films, by = "film")

num_vars <- merged %>%
  select(run_time, rotten_tomatoes, metacritic, critics_choice)

corr_matrix <- cor(num_vars, use = "pairwise.complete.obs")

ggcorrplot(corr_matrix,
           lab = TRUE,
           type = "lower",
           hc.order = TRUE,
           colors = c("blue", "white", "red"))
```



The correlation matrix shows strong and positive associations between all three critic‑based rating systems.

Rotten Tomatoes and Metacritic: r = 0.80

Rotten Tomatoes and Critics’ Choice: r = 0.85

Metacritic and Critics’ Choice: r = 0.86

These coefficients indicate that films rated highly by one platform tend to receive similarly high scores on the others. The strength of the correlations (all above 0.80) suggests that the different critic aggregators evaluate Pixar films in a highly consistent manner.

The correlations between runtime and the three critics’ rating variables (Rotten Tomatoes, Metacritic, Critics’ Choice) were all weak and negative (r = –0.22, –0.13, –0.13).
These values indicate a very weak inverse relationship, meaning that longer films tend to receive slightly lower critic scores, but the effect is minimal.
Given the small magnitude of the correlations, these relationships are not statistically significant.
Overall, runtime does not appear to meaningfully influence critical reception for Pixar films.


# Boxplot by other scores
## Cinema_score and rotten_tomatoes
```{r}
public_response %>%
  ggplot(aes(x = cinema_score, y = rotten_tomatoes)) +
  geom_boxplot(fill = "#E69F00") +
  labs(title = "Rotten Tomatoes by CinemaScore category",
       x = "CinemaScore",
       y = "RT score")
kruskal.test(rotten_tomatoes ~ cinema_score, data = public_response)

library(FSA)

dunnTest(x = public_response$rotten_tomatoes,
         g = as.factor(public_response$cinema_score),
         method = "bonferroni")
```



To examine whether Rotten Tomatoes scores differ across audience ratings from CinemaScore, we ran a Kruskal–Wallis test. The result was significant (χ² = 6.29, df = 2, p = 0.043), indicating that at least one CinemaScore category is associated with different critics’ ratings.

A Dunn post‑hoc test with Holm adjustment revealed that the only statistically meaningful difference was between films rated A‑ and A+ (p = 0.067 after adjustment, marginally non‑significant but clearly the strongest contrast). This suggests that movies receiving the highest audience grade (A+) tend to have higher Rotten Tomatoes scores compared to those receiving A‑, while the difference between A and the other categories was small and not statistically significant.

Overall, the results indicate that audience grades and professional critic scores are partially aligned: the very best audience-rated films (A+) also tend to receive higher professional critic scores, whereas differences between A and A‑ are less pronounced.



# Metacritic vs CinemaScore
```{r}
public_response %>%
  ggplot(aes(x = cinema_score, y = metacritic, fill = cinema_score)) +
  geom_boxplot() +
  labs(title = "Metacritic by CinemaScore",
       x = "CinemaScore", y = "Metacritic score") +
  theme_minimal()

kruskal.test(metacritic ~ cinema_score, data = public_response)
```




To examine whether Metacritic scores differ across CinemaScore categories, a Kruskal–Wallis rank‑sum test was conducted. The test result was:

χ²(2) = 5.04,

p = 0.080

Since the p‑value is above the conventional 0.05 significance threshold, there is no statistically significant difference in Metacritic scores between the CinemaScore groups.

Although the test statistic suggests a possible trend toward differences, the evidence is not strong enough to conclude that audience grades (CinemaScore) are associated with systematically higher or lower Metacritic ratings in this dataset.

# Critics' Choice vs CinemaScore

```{r}
public_response %>%
  ggplot(aes(x = cinema_score, y = critics_choice, fill = cinema_score)) +
  geom_boxplot() +
  labs(title = "Critics' Choice by CinemaScore",
       x = "CinemaScore", y = "Critics' Choice score") +
  theme_minimal()

kruskal.test(critics_choice ~ cinema_score, data = public_response)
```



To examine whether Critics’ Choice scores differ across audience ratings (CinemaScore groups), a Kruskal–Wallis test was performed. The result was not statistically significant (χ²(2) = 4.94, p = 0.085), indicating that there is no strong evidence of differences in Critics’ Choice ratings between the CinemaScore categories included in the analysis. Because the global test was non‑significant, no post‑hoc comparisons were conducted. Overall, Critics’ Choice evaluations appear relatively consistent across films receiving different audience grades.


Overall, the results suggest a weak but detectable relationship between CinemaScore and Rotten Tomatoes evaluations, while Metacritic and Critics Choice scores appear largely independent of audience‑assigned CinemaScore grades.




## 3. Modellek építése
- legalább **két modell** ugyanarra az outcome változóra
  - pl. lineáris modell, logisztikus, random forest, stb.
- modell-összehasonlítás
  - AIC, RMSE, accuracy stb.
- rövid konklúzió: melyik jobb és miért?



# Model results in tables
```{r}
library(dplyr)
library(randomForest)
library(caret)
library(Metrics)

# ---- 1. Data preparation ----
model_data <- public_response %>%
  left_join(pixar_films, by = "film") %>%
  select(rotten_tomatoes, run_time, release_year, metacritic, critics_choice) %>%
  na.omit()

# ---- 2. Linear Regression Model ----
model_lm <- lm(rotten_tomatoes ~ run_time + release_year + metacritic + critics_choice,
               data = model_data)

pred_lm <- predict(model_lm, model_data)

lm_results <- data.frame(
  Model = "Linear Regression",
  RMSE  = rmse(model_data$rotten_tomatoes, pred_lm),
  MAE   = mae(model_data$rotten_tomatoes, pred_lm),
  R2    = caret::R2(pred_lm, model_data$rotten_tomatoes),
  AIC   = AIC(model_lm)
)

# ---- 3. Random Forest Model ----
set.seed(123)
model_rf <- randomForest(
  rotten_tomatoes ~ run_time + release_year + metacritic + critics_choice,
  data = model_data,
  ntree = 500
)

pred_rf <- predict(model_rf, model_data)

rf_results <- data.frame(
  Model = "Random Forest",
  RMSE  = rmse(model_data$rotten_tomatoes, pred_rf),
  MAE   = mae(model_data$rotten_tomatoes, pred_rf),
  R2    = caret::R2(pred_rf, model_data$rotten_tomatoes),
  AIC   = NA       # AIC RF-re nem értelmezhető
)

# ---- 4. Model Comparison Table ----
model_comparison <- bind_rows(lm_results, rf_results)

# ---- 5. Output ----
lm_results
rf_results
model_comparison
```



Model Specification and Comparison
To evaluate how well different models predict Rotten Tomatoes scores, two regression approaches were fitted: a linear regression model and a random forest regression model.
Both models use the same outcome variable and the same set of predictors, allowing a fair comparison.

Outcome Variable
Rotten Tomatoes score (numeric, 0–100)
Represents aggregated critic ratings of each Pixar film.

Predictor Variables
The following explanatory variables were included in both models:

run_time — film length in minutes

release_year — year of film release

metacritic — Metacritic critic score (0–100)

critics_choice — Critics’ Choice score (0–100)

Hypothesis
We expect that:

Higher critic‑based numerical scores (Metacritic, Critics' Choice) will positively predict Rotten Tomatoes scores.

Release year and runtime may contribute modestly, but with weaker effects.

A non‑linear model (random forest) should outperform the linear model, because relationships among critic scores may not be strictly linear.

1. Linear Regression — Model Performance
Metric	Value
RMSE	6.42
MAE	4.85
R²	0.798
AIC	149.68
The linear model explains about 79–80% of the variance in Rotten Tomatoes scores.
Prediction accuracy is moderate, and the linear structure may not fully capture complex interactions between critic rating systems.

2. Random Forest — Model Performance
Metric	Value
RMSE	5.12
MAE	2.98
R²	0.925
The random forest model shows a substantial improvement:

Error values decrease noticeably

R² exceeds 0.92, indicating a very strong predictive fit
Its ability to learn non‑linear patterns makes it well‑suited for this task.

3. Model Comparison
Metric	Linear Regression	Random Forest
RMSE	6.42	5.12
MAE	4.85	2.98
R²	0.798	0.925
Conclusion
The random forest model clearly outperforms the linear regression model across all evaluation metrics.
Its superior performance likely comes from its ability to capture non‑linear relationships and complex interactions among the rating variables.

Therefore, the random forest model is the preferred approach for predicting Rotten Tomatoes scores in this dataset.


```{r}

```



```{r}

```


```{r}

```




## 4. Modellek értékelése
- **assumption check + residual diagnostics**
  - pl. QQ plot, residuals vs fitted plot
- értelmezés: mi látszik, rendben vannak-e a feltételek?

```{r}


```

## 5. Következtetések / Report
- összefoglalás emberi nyelven
- mik a tanulságok?
- mit találtál érdekesnek?

```{r}


```

## 6. Kreatív rész (ha szeretnél)
- extra vizualizációk
- extra kérdések
- saját ötletek

```{r}


```
