---
title: "TidyTuesday 2025-03-11 - Pixar films"
author: "Vikár Míra"
output:
  html_document:
    df_print: paged
    self_contained: false
---

In this project, I examine data from Pixar films.  
My goal is to explore data patterns (EDA) and then build, compare, and evaluate at least two models.  
The data analysis is fully reproducible, and all code is available in Rmd.

# Load packages

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(tidyverse)
library(readr)
```

```{r}
# Importing the Pixar dataset directly from GitHub

pixar_films <- readr::read_csv(
  "https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-03-11/pixar_films.csv"
)

public_response <- readr::read_csv(
  "https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-03-11/public_response.csv"
)
```

# Pixar dataset head and glimpse
```{r}
head(pixar_films)
head(public_response)
knitr::kable(head(pixar_films))
knitr::kable(head(public_response))
glimpse(pixar_films)
glimpse(public_response)
```

## 1. Data cleaning
### 1.1 Examining variable types and missing data
```{r}
glimpse(pixar_films)
summary(pixar_films)

glimpse(public_response)
summary(public_response)
```

### 1.2 Exemination of missing values
```{r}
public_response %>%
  filter(is.na(critics_choice) | is.na(rotten_tomatoes))
```



Data Overview and Initial Cleaning Steps
Before starting the analysis, I examined the dataset to understand its structure and identify potential data quality issues. The dataset contains 24 Pixar films and includes the following variables:

film (character) – The title of the movie.

rotten_tomatoes (numeric) – Percentage score from Rotten Tomatoes.

metacritic (numeric) – Metacritic critic score (0–100).

cinema_score (character) – Audience grade (A, A+, etc.).

critics_choice (numeric) – Critics’ Choice rating (0–100).

Summary of Key Findings
A first look using glimpse() and summary() revealed the following:

All variables were loaded correctly with the expected data types.

cinema_score is stored as a character variable, not numeric, because it contains letter-based grades (e.g., A, A+).

Some variables include missing values (NA):

critics_choice: 3 missing values

metacritic: 1 missing value

cinema_score: 1 missing value

I also inspected rows containing missing data. These came exclusively from the film “Luca” and two early films where Critics Choice ratings were not available.

Next Step: Data Cleaning Plan
Since the dataset is small and missing values are minimal, carefully handling NAs is important. 

### 1.3 Handling Missing Data

Before proceeding with the exploratory data analysis (EDA), it is essential to
examine the dataset for missing values. Missing data can influence summary
statistics, distort visualisations, and potentially introduce bias during
model building. Therefore, identifying and addressing these values is a
necessary first step.

Treatment Strategy
Because the missingness is low in proportion and the affected variables serve
different analytical purposes, we apply the following approach:

cinema_score:
We convert this variable into an ordered factor with levels
A- < A < A+. The single missing value is left as NA, as imputing an
ordered rating would introduce artificial information.

### 1.4 Convert cinema_score to an ordered factor
```{r}
public_response <- public_response %>%
  mutate(
    cinema_score = factor(
      cinema_score,
      levels = c("A-", "A", "A+"),   
      ordered = TRUE
    )
  )

```



critics_choice:
Since this is a numerical rating derived from awards evaluation, imputing
a synthetic score would not be justified. These values are therefore
retained as NA and will be handled appropriately during later modelling
(e.g., filtering out incomplete rows or using models that support missingness).

Verification
After these transformations, we confirm that the missing values remain only in
the intended variables:

### 1.5 Checking the type of variable
```{r}
str(public_response$cinema_score)
```
### 1.6 Checking values and sequences
```{r}
levels(public_response$cinema_score)
```

### 1.7 Check missing values again
```{r}
public_response %>% summarise(across(everything(), ~ sum(is.na(.))))
```  



This ensures that the dataset is clean, consistent, and properly structured
for the next steps in the analysis.

## 2. Exploratory Data Analysis (EDA)

### 2.1 Distribution of run_time
```{r}
ggplot(pixar_films, aes(x = run_time)) +
  geom_histogram(fill = "#0073C2FF", color = "white", bins = 10) +
  labs(title = "Distribution of Film Runtime",
       x = "Runtime (minutes)", y = "Count")

```


### 2.2 Basic descriptive statistics for the length of the film
```{r}
summary_stats <- pixar_films %>% 
  summarise(
    mean_run_time = mean(run_time, na.rm = TRUE),
    median_run_time = median(run_time, na.rm = TRUE),
    sd_run_time = sd(run_time, na.rm = TRUE),
    min_run_time = min(run_time, na.rm = TRUE),
    max_run_time = max(run_time, na.rm = TRUE)
  )

summary_stats

```



The distribution of film run times is approximately normal, with most movies lasting between 80 and 120 minutes. The mean run time is 104.8 minutes (median = 102 minutes, SD = 16.8), indicating that typical Pixar films cluster closely around a roughly 100‑minute length. The minimum run time is 81 minutes, while two substantially longer films extend the range up to 155 minutes. These long outliers create a mild right skew in an otherwise symmetric distribution.

### 2.3 run_time vs Rotten Tomatoes, Metacritic, Critics Choice

```{r}
library(dplyr)
library(tidyr)

ratings_long <- public_response %>%
  pivot_longer(cols = c(rotten_tomatoes, metacritic, critics_choice),
               names_to = "rating_type",
               values_to = "rating")

ggplot(ratings_long %>% left_join(pixar_films, by = "film"),
       aes(x = run_time, y = rating)) +
  geom_point(size = 3) +
  geom_smooth(method = "lm", se = FALSE, color = "black") +
  facet_wrap(~ rating_type, scales = "free_y") +
  labs(
    title = "Runtime vs Ratings (Multiple Sources)",
    x = "Runtime (minutes)",
    y = "Rating"
  ) +
  theme_minimal()


```


### 2.4 run_time vs Rotten Tomatoes, Metacritic, Critics Choice Correlations
```{r}
df <- left_join(pixar_films, public_response, by = "film")

cor.test(df$run_time, df$rotten_tomatoes)
cor.test(df$run_time, df$metacritic)
cor.test(df$run_time, df$critics_choice)

```



Correlation analyses were conducted to examine the association between film run time and three measures of critical response (Rotten Tomatoes, Metacritic, Critics’ Choice). Across all three comparisons, results indicated no statistically significant relationship. Run time showed a weak, negative, and non-significant correlation with Rotten Tomatoes scores (r = –0.22, p = .315), Metacritic scores (r = –0.13, p = .564), and Critics’ Choice ratings (r = –0.13, p = .577). The confidence intervals for all estimates included zero, suggesting that film length is not meaningfully associated with critical evaluations. These findings also align with the scatterplots, which show no clear linear trend.

### 2.5 Frequency of film ratings (film_rating)

```{r}
clean_films <- pixar_films %>%
  filter(film_rating %in% c("G", "PG"))
unique(clean_films$film_rating)

```



```{r}
  ggplot(clean_films, aes(x = film_rating)) +
  geom_bar(fill = "#E69F00") +
  labs(title = "Distribution of Film Ratings",
       x = "Rating",
       y = "Count")

```


### 2.6 Frequency of film ratings and its association with running time
```{r}
#frequency table
table(clean_films$film_rating)

#relative frequency
prop.table(table(clean_films$film_rating))

#Are the running times different between G and PG movies?
wilcox.test(run_time ~ film_rating, data = clean_films)


```



After filtering the dataset to include only the meaningful MPAA categories (“G” and “PG”), I examined whether film rating is associated with differences in runtime. A Wilcoxon rank-sum test showed no statistically significant difference between the two groups (W = 64, p = 0.975). This indicates that Pixar’s G‑rated and PG‑rated movies do not differ in their typical runtimes. In practice, this suggests that film length is unrelated to the assigned age rating, and Pixar maintains similar durations across both categories.


### 2.7 Changes in run time (film length) over time

```{r}
library(lubridate)

pixar_films <- pixar_films %>%
  mutate(release_year = year(release_date))
```


```{r}
model_runtime_year <- lm(run_time ~ release_year, data = pixar_films)
summary(model_runtime_year)

plot(model_runtime_year)

pixar_films %>%
  ggplot(aes(x = release_date, y = run_time)) +
  geom_point(color = "#0072B2", size = 3) +
  geom_smooth(method = "lm", se = TRUE, color = "red") +
  labs(
    title = "Film length in time",
    x = "Date of publication",
    y = "Length (minutes)"
  )
```




A simple linear regression was performed to examine whether Pixar film runtimes have changed across release years. The model shows a significant positive relationship between release year and runtime (β = 1.078, p = 0.0098). This means that, on average, Pixar movies have become about 1.1 minutes longer each year.

The model explains approximately 25.6% of the variance in runtimes (R² = 0.256), which indicates a moderate relationship: release year is a meaningful, but not exclusive, predictor of runtime. The residual standard error (14.78 minutes) suggests that runtimes still vary substantially beyond the trend captured by the model.

We should also consider that The scatterplot suggests a very mild upward trend in movie runtimes over time, but the pattern is far from linear. The smoothing curve is wavy, implying that runtimes fluctuate rather than follow a steady increase. Three clear outliers appear at fitted values around 98, 115, and 118 minutes, corresponding to unusually short or long movies compared with their release-year peers.
The residuals-vs-fitted plot shows non‑random structure: the loess curve bends several times, indicating model misspecification and suggesting that a linear model does not fit the data well. Additionally, the Q–Q plot shows noticeable deviations from normality at both ends, driven largely by the same outliers identified earlier.

Overall, the results support the conclusion that Pixar movies tend to get longer over time, although year alone does not account for all runtime differences.



### 2.8 How much does time matter when it comes to movie ratings (G vs. PG)?
```{r}
# Add release_year
clean_films <- clean_films %>%
  mutate(release_year = lubridate::year(release_date))

# Convert rating to binary (PG = 1, G = 0)
clean_films$rating_binary <- ifelse(clean_films$film_rating == "PG", 1, 0)

# Logistic regression: does rating depend on release year?
rating_model <- glm(rating_binary ~ release_year, 
                    data = clean_films, 
                    family = binomial)

summary(rating_model)

pixar_films %>%
  ggplot(aes(x = release_date, fill = film_rating)) +
  geom_bar() +
  labs(
    title = "Movie ratings over time",
    x = "Date of publication",
    y = "Number of pieces"
  )
```



The plot shows how movie ratings have changed over time across the Pixar dataset. Early releases are almost exclusively rated G, but from the late 2000s onward PG-rated films appear more frequently, and they gradually become the dominant category. A few films in the 2020s have missing or “Not Rated” labels, but these represent isolated cases rather than a trend.

To quantify this visually observable shift, I fitted a logistic regression predicting the probability of a movie being PG based on its release year. The model shows a significant positive effect of release year (p = 0.0246), meaning that movies released later are statistically more likely to receive a PG rating.

### 2.9 Trends in critical reviews over time

```{r}
public_response2 <- public_response %>%
  left_join(pixar_films, by = "film") %>%
  mutate(release_year = as.numeric(format(release_date, "%Y")))

rt_model <- lm(rotten_tomatoes ~ release_year, data = public_response2)
summary(rt_model)

public_response %>%
  left_join(pixar_films, by = "film") %>%
  ggplot(aes(x = release_date, y = rotten_tomatoes)) +
  geom_point(color = "#E69F00", size = 3) +
  geom_smooth(method = "lm", se = TRUE, color = "red") +
  labs(
    title = "Rotten Tomatoes score over time",
    x = "Date of publication",
    y = "RT score"
  )
```




The scatterplot shows Rotten Tomatoes scores across release years, together with a fitted linear trend. The distribution of points suggests no strong upward or downward pattern in critical reception over time.

The linear regression model confirms this visually observed result. The slope of the regression line is small and negative (β = –0.40), but not statistically significant (p = 0.33). The model explains almost none of the variance in the scores (adjusted R² ≈ 0). This indicates that release year is not a meaningful predictor of Rotten Tomatoes ratings in this dataset.

Overall, the analysis suggests that Pixar films have maintained relatively stable critical reception over time, without a clear long-term improvement or decline.


### 2.10 Examination of the relationship between assessments and further correlation studies

#### Join the two datasets by film name
```{r}
library(dplyr)
library(ggplot2)


pixar_joined <- public_response %>%
  left_join(pixar_films, by = "film")

```



#### Histogram for all 3 numeric critic scores
```{r}
public_response %>%
  select(rotten_tomatoes, metacritic, critics_choice) %>%
  gather(key = "source", value = "score") %>%
  ggplot(aes(x = score)) +
  geom_histogram(bins = 10, fill = "#E69F00", color = "black") +
  facet_wrap(~ source, scales = "free") +
  labs(title = "Distribution of critic scores",
       x = "Score",
       y = "Count")
```




Across all three critic-based rating variables (Rotten Tomatoes, Metacritic, Critics’ Choice), the score distributions are right‑skewed, indicating that most Pixar films tend to receive high critical evaluations.

Rotten Tomatoes shows the strongest skew: the majority of films score above 90 points, while only a few titles fall below 60 points. Metacritic scores are also skewed but more moderately, with most films positioned between 70 and 90 points. Critics’ Choice ratings follow a similar pattern, with almost all films receiving scores above 85 points, and only a small number falling into the mid‑range (50–70 points).

Overall, the distributions suggest that Pixar films consistently achieve high critical acclaim, with very few low-rated outliers.



#### Rotten Tomatoes vs Metacritic
```{r}
ggplot(pixar_joined, aes(x = rotten_tomatoes, y = metacritic)) +
  geom_point(color = "#E69F00", size = 3) +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  labs(title = "Rotten Tomatoes vs Metacritic",
       x = "Rotten Tomatoes score",
       y = "Metacritic score")
```



#### Rotten Tomatoes vs Critics Choice
```{r}
ggplot(pixar_joined, aes(x = rotten_tomatoes, y = critics_choice)) +
  geom_point(color = "#56B4E9", size = 3) +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  labs(title = "Rotten Tomatoes vs Critics Choice",
       x = "Rotten Tomatoes score",
       y = "Critics Choice score")
```

#### Metacritic vs Critics Choice
```{r}
ggplot(pixar_joined, aes(x = metacritic, y = critics_choice)) +
  geom_point(color = "#009E73", size = 3) +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  labs(title = "Metacritic vs Critics Choice",
       x = "Metacritic score",
       y = "Critics Choice score")
```


#### Calculation of correlations between numerical evaluations
```{r}
numeric_scores <- pixar_joined %>%
  select(rotten_tomatoes, metacritic, critics_choice)

cor_matrix <- cor(numeric_scores, use = "complete.obs")

cor_matrix
```


#### Correlation heatmap
```{r}
install.packages("ggcorrplot")
library(ggcorrplot)

merged <- public_response %>%
  left_join(pixar_films, by = "film")

num_vars <- merged %>%
  select(run_time, rotten_tomatoes, metacritic, critics_choice)

corr_matrix <- cor(num_vars, use = "pairwise.complete.obs")

ggcorrplot(corr_matrix,
           lab = TRUE,
           type = "lower",
           hc.order = TRUE,
           colors = c("blue", "white", "red"))
```



The correlation matrix shows strong and positive associations between all three critic‑based rating systems.

Rotten Tomatoes and Metacritic: r = 0.80

Rotten Tomatoes and Critics’ Choice: r = 0.85

Metacritic and Critics’ Choice: r = 0.86

These coefficients indicate that films rated highly by one platform tend to receive similarly high scores on the others. The strength of the correlations (all above 0.80) suggests that the different critic aggregators evaluate Pixar films in a highly consistent manner.

The correlations between runtime and the three critics’ rating variables (Rotten Tomatoes, Metacritic, Critics’ Choice) were all weak and negative (r = –0.22, –0.13, –0.13).
These values indicate a very weak inverse relationship, meaning that longer films tend to receive slightly lower critic scores, but the effect is minimal.
Given the small magnitude of the correlations, these relationships are not statistically significant.
Overall, runtime does not appear to meaningfully influence critical reception for Pixar films.


#### Boxplot by other scores
##### Cinema_score and rotten_tomatoes
```{r}
public_response %>%
  ggplot(aes(x = cinema_score, y = rotten_tomatoes)) +
  geom_boxplot(fill = "#E69F00") +
  labs(title = "Rotten Tomatoes by CinemaScore category",
       x = "CinemaScore",
       y = "RT score")
kruskal.test(rotten_tomatoes ~ cinema_score, data = public_response)

library(FSA)

dunnTest(x = public_response$rotten_tomatoes,
         g = as.factor(public_response$cinema_score),
         method = "bonferroni")
```



To examine whether Rotten Tomatoes scores differ across audience ratings from CinemaScore, we ran a Kruskal–Wallis test. The result was significant (χ² = 6.29, df = 2, p = 0.043), indicating that at least one CinemaScore category is associated with different critics’ ratings.

A Dunn post‑hoc test with Holm adjustment revealed that the only statistically meaningful difference was between films rated A‑ and A+ (p = 0.067 after adjustment, marginally non‑significant but clearly the strongest contrast). This suggests that movies receiving the highest audience grade (A+) tend to have higher Rotten Tomatoes scores compared to those receiving A‑, while the difference between A and the other categories was small and not statistically significant.

Overall, the results indicate that audience grades and professional critic scores are partially aligned: the very best audience-rated films (A+) also tend to receive higher professional critic scores, whereas differences between A and A‑ are less pronounced.



##### Metacritic vs CinemaScore
```{r}
public_response %>%
  ggplot(aes(x = cinema_score, y = metacritic, fill = cinema_score)) +
  geom_boxplot() +
  labs(title = "Metacritic by CinemaScore",
       x = "CinemaScore", y = "Metacritic score") +
  theme_minimal()

kruskal.test(metacritic ~ cinema_score, data = public_response)
```




To examine whether Metacritic scores differ across CinemaScore categories, a Kruskal–Wallis rank‑sum test was conducted. The test result was:

χ²(2) = 5.04,

p = 0.080

Since the p‑value is above the conventional 0.05 significance threshold, there is no statistically significant difference in Metacritic scores between the CinemaScore groups.

Although the test statistic suggests a possible trend toward differences, the evidence is not strong enough to conclude that audience grades (CinemaScore) are associated with systematically higher or lower Metacritic ratings in this dataset.

##### Critics' Choice vs CinemaScore

```{r}
public_response %>%
  ggplot(aes(x = cinema_score, y = critics_choice, fill = cinema_score)) +
  geom_boxplot() +
  labs(title = "Critics' Choice by CinemaScore",
       x = "CinemaScore", y = "Critics' Choice score") +
  theme_minimal()

kruskal.test(critics_choice ~ cinema_score, data = public_response)
```



To examine whether Critics’ Choice scores differ across audience ratings (CinemaScore groups), a Kruskal–Wallis test was performed. The result was not statistically significant (χ²(2) = 4.94, p = 0.085), indicating that there is no strong evidence of differences in Critics’ Choice ratings between the CinemaScore categories included in the analysis. Because the global test was non‑significant, no post‑hoc comparisons were conducted. Overall, Critics’ Choice evaluations appear relatively consistent across films receiving different audience grades.


Overall, the results suggest a weak but detectable relationship between CinemaScore and Rotten Tomatoes evaluations, while Metacritic and Critics Choice scores appear largely independent of audience‑assigned CinemaScore grades.




## 3. Modellek építése

### 3.1 Model results in tables
```{r}
library(dplyr)
library(randomForest)
library(caret)
library(Metrics)

# ---- 1. Data preparation ----
model_data <- public_response %>%
  left_join(pixar_films, by = "film") %>%
  select(rotten_tomatoes, run_time, release_year, metacritic, critics_choice) %>%
  na.omit()

# ---- 2. Linear Regression Model ----
model_lm <- lm(rotten_tomatoes ~ run_time + release_year + metacritic + critics_choice,
               data = model_data)

pred_lm <- predict(model_lm, model_data)

lm_results <- data.frame(
  Model = "Linear Regression",
  RMSE  = rmse(model_data$rotten_tomatoes, pred_lm),
  MAE   = mae(model_data$rotten_tomatoes, pred_lm),
  R2    = caret::R2(pred_lm, model_data$rotten_tomatoes),
  AIC   = AIC(model_lm)
)

# ---- 3. Random Forest Model ----
set.seed(123)
model_rf <- randomForest(
  rotten_tomatoes ~ run_time + release_year + metacritic + critics_choice,
  data = model_data,
  ntree = 500
)

pred_rf <- predict(model_rf, model_data)

rf_results <- data.frame(
  Model = "Random Forest",
  RMSE  = rmse(model_data$rotten_tomatoes, pred_rf),
  MAE   = mae(model_data$rotten_tomatoes, pred_rf),
  R2    = caret::R2(pred_rf, model_data$rotten_tomatoes),
  AIC   = NA       # AIC RF-re nem értelmezhető
)

# ---- 4. Model Comparison Table ----
model_comparison <- bind_rows(lm_results, rf_results)

# ---- 5. Output ----
lm_results
rf_results
model_comparison
```



Model Specification and Comparison
To evaluate how well different models predict Rotten Tomatoes scores, two regression approaches were fitted: a linear regression model and a random forest regression model.
Both models use the same outcome variable and the same set of predictors, allowing a fair comparison.

Outcome Variable
Rotten Tomatoes score (numeric, 0–100)
Represents aggregated critic ratings of each Pixar film.

Predictor Variables
The following explanatory variables were included in both models:

run_time — film length in minutes

release_year — year of film release

metacritic — Metacritic critic score (0–100)

critics_choice — Critics’ Choice score (0–100)

Hypothesis
We expect that:

Higher critic‑based numerical scores (Metacritic, Critics' Choice) will positively predict Rotten Tomatoes scores.

Release year and runtime may contribute modestly, but with weaker effects.

A non‑linear model (random forest) should outperform the linear model, because relationships among critic scores may not be strictly linear.

1. Linear Regression — Model Performance
Metric	Value
RMSE	6.42
MAE	4.85
R²	0.798
AIC	149.68
The linear model explains about 79–80% of the variance in Rotten Tomatoes scores.
Prediction accuracy is moderate, and the linear structure may not fully capture complex interactions between critic rating systems.

2. Random Forest — Model Performance
Metric	Value
RMSE	5.12
MAE	2.98
R²	0.925
The random forest model shows a substantial improvement:

Error values decrease noticeably

R² exceeds 0.92, indicating a very strong predictive fit
Its ability to learn non‑linear patterns makes it well‑suited for this task.

3. Model Comparison
Metric	Linear Regression	Random Forest
RMSE	6.42	5.12
MAE	4.85	2.98
R²	0.798	0.925
Conclusion
The random forest model clearly outperforms the linear regression model across all evaluation metrics.
Its superior performance likely comes from its ability to capture non‑linear relationships and complex interactions among the rating variables.

Therefore, the random forest model is the preferred approach for predicting Rotten Tomatoes scores in this dataset.


## 4. Eveluation of Models
### 4.1 Assumption Checks & Diagnostics

```{r}
## 4. Model Evaluation: Assumption Checks & Diagnostics


library(dplyr)
library(ggplot2)
library(randomForest)
library(caret)
library(Metrics)

# --- Prepare data (same as before) ---
model_data <- public_response %>%
  left_join(pixar_films, by = "film") %>%
  select(rotten_tomatoes, run_time, release_year, metacritic, critics_choice) %>%
  na.omit()

# --- Fit models again (same formula) ---
model_lm <- lm(rotten_tomatoes ~ run_time + release_year + metacritic + critics_choice,
               data = model_data)

set.seed(123)
model_rf <- randomForest(
  rotten_tomatoes ~ run_time + release_year + metacritic + critics_choice,
  data = model_data,
  ntree = 500
)


## 4.1 LINEAR MODEL ASSUMPTION CHECKS

# ---- Residuals vs Fitted ----
plot(model_lm, which = 1)

# ---- Normal Q-Q plot ----
plot(model_lm, which = 2)

# ---- Scale-Location plot (homoscedasticity) ----
plot(model_lm, which = 3)

# ---- Residuals vs Leverage ----
plot(model_lm, which = 5)

# Additional ggplot version for the report:
lm_residuals <- data.frame(
  fitted = fitted(model_lm),
  residuals = residuals(model_lm)
)

ggplot(lm_residuals, aes(x = fitted, y = residuals)) +
  geom_point(color = "steelblue") +
  geom_hline(yintercept = 0, linetype = "dashed") +
  theme_minimal() +
  labs(
    title = "Linear Model — Residuals vs Fitted",
    x = "Fitted values",
    y = "Residuals"
  )


## 4.2 RANDOM FOREST DIAGNOSTICS


# ---- Residuals vs Fitted ----
rf_residuals <- data.frame(
  fitted = predict(model_rf, model_data),
  residuals = model_data$rotten_tomatoes - predict(model_rf, model_data)
)

ggplot(rf_residuals, aes(x = fitted, y = residuals)) +
  geom_point(color = "darkgreen") +
  geom_hline(yintercept = 0, linetype = "dashed") +
  theme_minimal() +
  labs(
    title = "Random Forest — Residuals vs Fitted",
    x = "Fitted values",
    y = "Residuals"
  )

# ---- Variable importance ----
varImpPlot(model_rf)


## 4.3 MODEL COMPARISON — Residual Distributions
# Collect residuals
diagnostics <- data.frame(
  model = rep(c("Linear Regression", "Random Forest"),
              each = nrow(model_data)),
  residuals = c(
    model_data$rotten_tomatoes - predict(model_lm, model_data),
    model_data$rotten_tomatoes - predict(model_rf, model_data)
  )
)

ggplot(diagnostics, aes(x = model, y = residuals, fill = model)) +
  geom_boxplot(alpha = 0.7) +
  scale_fill_manual(values = c("#0072B2", "#009E73")) +
  theme_minimal() +
  labs(
    title = "Comparison of Residual Distributions",
    x = "",
    y = "Residuals"
  )

```





Diagnostic Evaluation of the Linear Regression Model
To assess the validity of the linear regression model and the extent to which its statistical assumptions are met, four standard diagnostic plots were examined: Residuals vs Fitted, Normal Q–Q, Scale–Location, and Residuals vs Leverage. Together, these plots allow evaluation of linearity, homoscedasticity, normality of residuals, and the influence of individual observations.

1. Residuals vs Fitted
This plot is used to check linearity and homoscedasticity.
In the present model, the points show a systematic curvature, including a noticeable downward bend around a fitted value of ~90. This suggests that the model does not fully capture nonlinear patterns in the data.
The presence of several clear outliers—most prominently one large negative residual near −22 and two additional moderate outliers around fitted values 77 and 88—further indicates misfit.
Although most observations cluster near the regression line, the pattern and curvature imply that the linearity assumption is violated.

2. Normal Q–Q Plot
This plot evaluates the normality of residuals.
Most points fall relatively close to the theoretical line, but the plot displays a mild S‑shape, and the same three outliers deviate strongly from the expected distribution.
Residual normality is therefore approximately met, but the heavy-tailed deviations introduced by the outliers represent a notable departure from ideal conditions.

3. Scale–Location Plot
This plot tests homoscedasticity, that is, whether the residual variance is constant across the range of fitted values.
The smoothed trend line shows several pronounced bends: an initial downward trend around fitted values near 80, followed by an upward shift, another strong downward break near 90, and a final steep rise around 95.
These patterns indicate that the spread of residuals changes along the fitted range, violating the constant-variance assumption.
The high-score end also shows a tighter clustering of points, consistent with non-uniform variance.

4. Residuals vs Leverage
This plot identifies influential points that might disproportionately affect the model estimates.
Three observations again stand out as clear outliers. The leverage values are generally low to moderate, but the combination of high residuals and non-trivial leverage suggests these points exert meaningful influence.
The central region of the plot has greater dispersion of residuals, while the rightmost portion contains very few observations, highlighting structural imbalance in the data.
Overall, the model exhibits influential observations that may distort coefficient estimates and overall model fit.

Summary
Across all diagnostics, the linear regression model shows multiple assumption violations:

Nonlinearity (curved pattern in residuals vs fitted)

Heteroscedasticity (changing variance)

Influential outliers (consistent across all plots)

While residual normality is only mildly affected, the more serious issues—nonlinearity and heteroscedasticity—suggest that a simple linear model does not adequately represent the underlying relationships in the data.
These diagnostics support the conclusion that more flexible, nonlinear approaches (such as the random forest model used in the analysis) are better suited to capturing the structure present in the dataset.

Random Forest – Residuals vs Fitted Plot 
The residuals vs fitted plot for the random forest model shows a strong concentration of points around fitted values close to 100, indicating that the model produces many predictions near the upper end of the Rotten Tomatoes score range. This clustering is typical when the true outcome values themselves are concentrated in a similar range, and it suggests that the model is capturing general patterns consistently for most observations.

A few outliers are visible, appearing as points noticeably distant from the main cluster. These outliers indicate individual films for which the model either substantially under‑ or over‑predicts the Rotten Tomatoes score. While random forest models are generally robust to irregularities and nonlinearities, these cases highlight observations whose characteristics differ from the broader pattern captured by the ensemble.

Importantly, unlike in linear regression diagnostics, the residuals vs fitted plot for random forests does not assess violations of linear model assumptions (e.g., homoscedasticity, linearity), because random forests do not rely on these assumptions. Instead, this plot is mainly used to identify:

potential systematic prediction bias (none strong appears here aside from the cluster at the top range),

heterogeneity in prediction accuracy across the score range,

and outliers where the model struggles.

Overall, the plot indicates that prediction errors remain small for most films, with only a few exceptional points showing large deviations. This is consistent with the strong quantitative performance of the random forest (high R², low RMSE and MAE).


Random Forest: Variable Importance
The random forest model provides a measure of variable importance based on the total decrease in node impurity contributed by each predictor across all trees. In the present analysis, four predictors were evaluated: release_year, run_time, critics_choice, and metacritic. Their relative importance values were as follows:

release_year: ~500

run_time: ~600

critics_choice: ~1200

metacritic: ~1400

These results indicate that metacritic contributed the most to reducing prediction error, followed closely by critics_choice. Both variables play a substantial role in how the model partitions the data and improve predictive accuracy. In contrast, run_time and release_year exhibit noticeably lower importance values, suggesting that they provide less unique information for predicting Rotten Tomatoes scores within this dataset.

Overall, the variable importance plot highlights that critical ratings (Metacritic and Critics’ Choice) are the dominant predictors in the random forest model, while film metadata (runtime and release year) contribute comparatively less to the model's performance.

Boxplot Diagnostics
The boxplots of the residuals further illustrate key differences in model behavior.
For the linear regression model, the residuals range approximately from –5 to +5, with the median located slightly above zero (around 0.2). While most residuals fall within a narrow central band, a noticeable outlier appears near –17, indicating a substantial underestimation for at least one observation. This suggests the presence of at least one influential point that the linear model fails to capture adequately.

The random forest model displays a narrower residual range, roughly from 0 to 3, with the median again close to 0.2, implying mild overall bias. Although the residual distribution is more compact—consistent with the model’s stronger predictive accuracy—several outliers remain visible, notably around –18 and –8, showing that the model still struggles with a few extreme cases, albeit far less frequently than the linear model.

Overall, the boxplots reinforce the conclusion that the random forest produces smaller, more tightly distributed residuals, while the linear regression model exhibits larger dispersion and more severe outliers, indicating weaker overall fit and greater sensitivity to influential observations.



### 4.2 Further model diagnostics

```{r}
# Cook's distance (identification of influential points)
plot(model_lm, which = 4)

# VIF Multicollinearity diagnostics
library(car)
vif(model_lm)

#Breusch–Pagan test – checking for heteroscedasticity
library(lmtest)
bptest(model_lm)

#Shapiro–Wilk test / Anderson–Darling – formalization of normal distribution
shapiro.test(residuals(model_lm))

#Autocorrelation test (Durbin–Watson)
dwtest(model_lm)

#Partial residual / component–plus–residual plots
crPlots(model_lm)

#Random forest model diagnostics
plot(model_rf)

```





Overall, the statistical diagnostic tests indicate that the linear regression model meets the key regression assumptions reasonably well.

The Breusch–Pagan test provides borderline evidence for heteroskedasticity (p = 0.069), suggesting that variance may not be perfectly constant, although the violation is not statistically significant at the 5% level.

The Shapiro–Wilk test shows no departure from residual normality (p = 0.252).

The Durbin–Watson statistic indicates no autocorrelation in the residuals (DW = 2.11, p = 0.46).

These results support the general adequacy of the linear model, although slight heteroskedasticity remains a potential concern and justifies the comparison with the random forest model.

Cook’s Distance
Cook’s distance was examined to identify observations with a disproportionately large influence on the fitted linear regression model. Three observations showed notably elevated Cook’s distance values (approximately 0.30, 0.40, and 0.90). While values above 1.0 are commonly considered clear indicators of problematic leverage, observations approaching this threshold may still exert substantial influence on coefficient estimates.

In this case, the point with a Cook’s distance close to 0.90 is of particular concern, as it suggests that removing this observation could lead to meaningful changes in the estimated regression parameters. The other two influential points (≈0.30 and ≈0.40) exert moderate influence but remain below typical cutoffs. Taken together, these diagnostics reinforce earlier findings from the residual and leverage plots: a small number of influential outliers are present, and their effects should be considered when interpreting the model.


Random Forest – Model Diagnostics
The additional diagnostic plot for the random forest model shows the relationship between predicted values and error estimates.
The curve starts with a steep vertical drop around zero, followed by a gradual upward trend that extends toward an error level of approximately 100. Beyond this point, the line stabilizes into a near‑horizontal segment. Overall, the curve appears slightly jagged, reflecting the inherent variability of bootstrap aggregation.

This pattern is typical for random forest models:

the initial steep section indicates high sensitivity to small prediction errors near zero,

the gradual upward drift suggests increasing error variance for larger fitted values,

while the final flattening indicates regions where prediction errors stabilize across trees.

The mild jaggedness is not problematic; it reflects the non‑parametric, ensemble‑based nature of the model rather than any violation of assumptions. Overall, the plot does not reveal structural issues that would undermine the reliability of the random forest predictions.


## 5. Conclusions

Overall, the analyses suggest that both the linear regression and the random forest model capture meaningful structure in the data, but they differ substantially in how they handle nonlinearity, outliers, and residual behavior.
The linear regression model shows generally acceptable statistical assumptions: the Breusch–Pagan test indicates no strong evidence of heteroskedasticity, the Shapiro–Wilk test suggests that residuals do not significantly deviate from normality, and the Durbin–Watson statistic points to the absence of problematic autocorrelation. However, multiple diagnostics consistently highlight three influential outliers, confirmed by Cook’s distance values approaching or exceeding commonly used thresholds. These observations materially affect the fitted trend and contribute to curvature, uneven residual spread, and instability in several diagnostic plots. It is also important to note that the dataset is relatively small, which limits the statistical power of assumption tests and increases the sensitivity of the linear model to influential points. This indicates that while the linear model is interpretable and statistically sound in its core assumptions, its fit is sensitive to influential data points and potential nonlinear patterns.

In contrast, the random forest model shows high predictive stability and strong performance across feature importance metrics. Its residuals cluster tightly around zero with fewer extreme deviations, and error patterns remain relatively consistent across boosted trees. This suggests that random forest is more robust to outliers and captures nonlinear relationships that the linear model cannot represent. The model’s behavior over iterations also indicates that predictive error stabilizes early, supporting the reliability of the final fit.

Taken together, these results highlight several key insights.
First, outliers matter: a small number of influential observations substantially distort the linear regression model while leaving the tree‑based model comparatively unaffected.
Second, nonlinearity is present: curvature in multiple diagnostic plots implies that linearity may not fully describe the underlying data-generating process.
Third, model choice depends on purpose: if interpretability and classical inference are priorities, linear regression remains useful but requires attention to influential observations. If predictive performance and robustness are the primary goals, the random forest model offers a clearer advantage.

In summary, the models provide complementary perspectives: the linear regression highlights where traditional assumptions begin to fail, and the random forest demonstrates how flexible, nonlinear methods can overcome those limitations. Both models contribute to understanding the structure of the data, but the tree-based approach appears better suited for prediction and handling irregularities such as outliers and nonlinear effects.




